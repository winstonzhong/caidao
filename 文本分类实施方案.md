# 抖音视频文案文本分类实施方案

## 一、方案概述

### 1.1 选定方案
**纯关键词匹配 + 同义词扩展（LLM自动生成）**

### 1.2 核心优势
| 优势 | 说明 |
|-----|------|
| 体积极小 | < 500KB（代码+数据），远低于10MB限制 |
| 零训练 | 无需准备训练数据，即开即用 |
| 动态可变 | 支持任意类别，输入类别描述即可生成分类器 |
| 可解释 | 匹配结果可展示命中了哪些关键词 |
| 速度快 | 毫秒级响应，适合批量处理 |
| 智能缓存 | 相同类别描述自动复用缓存，避免重复API调用 |

### 1.3 技术栈
- **关键词生成**：`helper_synonym.py`（基于LLM API批量生成同义词）
- **缓存机制**：MD5 Hash + 文件系统持久化
- **文本匹配**：Python标准库（re正则 + 字符串匹配）
- **分词**：可选jieba（如体积敏感可移除）
- **数据存储**：JSON文件（类别配置持久化）

---

## 二、系统架构

```
┌─────────────────────────────────────────────────────────┐
│                    输入层                                │
│  类别描述: "拼多多电商运营"                               │
│  文案: "#老陶电商 #拼多多运营 能发货的抓紧时间去报名..."  │
└─────────────────────────────────────────────────────────┘
                           │
           ┌───────────────┴───────────────┐
           ▼                               ▼
┌─────────────────────┐      ┌─────────────────────────────┐
│   Hash计算模块       │      │      文案解析模块            │
│                     │      │                             │
│  hash_id = md5(     │      │  • 提取hashtag (#话题)       │
│    "拼多多电商运营" │      │  • 提取正文内容              │
│  ) = "a3f7b2..."    │      │                             │
└──────────┬──────────┘      └─────────────────────────────┘
           │
           ▼
┌─────────────────────────────────────────────────────────┐
│                  缓存检查模块                            │
│  检查文件: cache/a3f7b2.../word_bank.json                │
│                                                         │
│  ┌─────────────────┐    ┌──────────────────────────┐   │
│  │  缓存存在？     │───→│  是: 直接读取本地文件    │   │
│  │                 │    │  否: 调用API生成并缓存   │   │
│  └─────────────────┘    └──────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
                           │
           ┌───────────────┴───────────────┐
           ▼                               ▼
┌─────────────────────┐      ┌─────────────────────────────┐
│   API调用生成       │      │      本地缓存读取            │
│                     │      │                             │
│  helper_synonym.    │      │  从 cache/a3f7b2.../        │
│  获取匹配词字典()   │      │  word_bank.json 读取        │
│                     │      │                             │
└──────────┬──────────┘      └──────────────┬──────────────┘
           │                                │
           └───────────────┬────────────────┘
                           ▼
┌─────────────────────────────────────────────────────────┐
│                  关键词匹配引擎                          │
│  • 精确匹配：词在文案中出现                               │
│  • hashtag加权：#话题 内的匹配权重×1.5                   │
│  • 权重累加：匹配到多个词时分数累加                       │
│  • 归一化：最终分数归一化到0-1                           │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────┐
│                   输出结果                               │
│  {                                                      │
│    'hash_id': 'a3f7b2...',                              │
│    '相似度': 0.85,                                      │
│    '匹配词': ['拼多多', '运营', '发货'],                 │
│    '匹配位置': ['文案正文', '#话题', '文案正文'],        │
│    '是否来自缓存': true                                 │
│  }                                                      │
└─────────────────────────────────────────────────────────┘
```

---

## 三、数据结构设计

### 3.1 类别词库缓存（CategoryWordBank）
```json
{
  "hash_id": "a3f7b2d8e5c9f1a4",
  "类别描述": "拼多多电商平台运营相关",
  "生成时间": "2026-02-25T15:45:00",
  "词库": {
    "拼多多": 1.0,
    "电商": 1.0,
    "运营": 1.0,
    "PDD": 0.6,
    "拼夕夕": 0.6,
    "pdd": 0.6,
    "店铺": 0.6,
    "发货": 0.6,
    "直通车": 0.6
  },
  "统计信息": {
    "核心词数": 3,
    "扩展词数": 6,
    "总词数": 9
  }
}
```

### 3.2 匹配结果（MatchResult）
```json
{
  "hash_id": "a3f7b2d8e5c9f1a4",
  "类别描述": "拼多多电商运营",
  "相似度": 0.88,
  "原始文案": "#老陶电商 #拼多多运营 能发货的抓紧时间去报名活动和打开直通车！",
  "匹配详情": {
    "核心词匹配": ["拼多多", "运营"],
    "扩展词匹配": ["发货"],
    "hashtag匹配": ["拼多多运营"]
  },
  "计分明细": {
    "基础分": 0.6,
    "hashtag加分": 0.28,
    "总分": 0.88
  },
  "是否来自缓存": true,
  "缓存路径": "cache/a3f7b2d8/word_bank.json"
}
```

---

## 四、核心模块设计

### 4.1 模块清单

| 模块名 | 文件 | 职责 |
|-------|------|------|
| 词库缓存管理器 | `category_cache_manager.py` | Hash计算、缓存检查、词库存储/读取 |
| 分类器核心 | `text_classifier.py` | 关键词匹配算法实现 |
| 批量处理器 | `batch_processor.py` | 批量处理JSON文件 |
| 评估工具 | `evaluator.py` | 准确率评估和调试 |

### 4.2 关键类设计

#### CategoryCacheManager（类别缓存管理器）
```python
# category_cache_manager.py
import hashlib
import json
import os
from typing import Optional, Dict
from pathlib import Path

class CategoryCacheManager:
    """
    类别词库缓存管理器
    
    职责：
    1. 计算类别描述的MD5 Hash作为唯一标识
    2. 检查本地缓存是否存在
    3. 如不存在，调用API生成并保存
    4. 如存在，直接读取本地缓存
    """
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    @staticmethod
    def compute_hash(description: str) -> str:
        """
        计算类别描述的MD5 Hash
        
        Args:
            description: 类别描述文本
        
        Returns:
            32位小写hex字符串
        """
        return hashlib.md5(description.encode('utf-8')).hexdigest()
    
    def get_cache_path(self, hash_id: str) -> Path:
        """获取缓存文件路径"""
        # 使用hash前8位创建子目录，避免单目录文件过多
        sub_dir = hash_id[:8]
        cache_subdir = self.cache_dir / sub_dir
        cache_subdir.mkdir(exist_ok=True)
        return cache_subdir / "word_bank.json"
    
    def exists(self, hash_id: str) -> bool:
        """检查缓存是否存在"""
        cache_path = self.get_cache_path(hash_id)
        return cache_path.exists()
    
    def load(self, hash_id: str) -> Optional[Dict[str, float]]:
        """
        从缓存加载词库
        
        Args:
            hash_id: 类别Hash ID
        
        Returns:
            词库字典，如不存在返回None
        """
        cache_path = self.get_cache_path(hash_id)
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data.get('词库', {})
        except Exception as e:
            print(f"加载缓存失败: {e}")
            return None
    
    def save(self, hash_id: str, description: str, word_bank: Dict[str, float]):
        """
        保存词库到缓存
        
        Args:
            hash_id: 类别Hash ID
            description: 原始类别描述
            word_bank: 词库字典
        """
        cache_path = self.get_cache_path(hash_id)
        
        # 分离核心词和扩展词（用于统计信息）
        核心词 = {k: v for k, v in word_bank.items() if v == 1.0}
        扩展词 = {k: v for k, v in word_bank.items() if v < 1.0}
        
        data = {
            "hash_id": hash_id,
            "类别描述": description,
            "生成时间": datetime.now().isoformat(),
            "词库": word_bank,
            "统计信息": {
                "核心词数": len(核心词),
                "扩展词数": len(扩展词),
                "总词数": len(word_bank)
            }
        }
        
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"词库已缓存: {cache_path}")
    
    def get_or_create(
        self, 
        description: str, 
        total_words: int = 20
    ) -> tuple:
        """
        获取或创建词库（核心方法）
        
        逻辑：
        1. 计算description的hash
        2. 检查缓存是否存在
        3. 存在则直接返回
        4. 不存在则调用API生成并缓存
        
        Args:
            description: 类别描述
            total_words: 词库大小
        
        Returns:
            (hash_id, word_bank, is_from_cache)
        """
        # 1. 计算Hash
        hash_id = self.compute_hash(description)
        
        # 2. 检查缓存
        word_bank = self.load(hash_id)
        if word_bank is not None:
            print(f"命中缓存: {hash_id[:8]}...")
            return hash_id, word_bank, True
        
        # 3. 缓存未命中，调用API生成
        print(f"缓存未命中，调用API生成: {hash_id[:8]}...")
        from helper_synonym import 获取匹配词字典
        word_bank = 获取匹配词字典(description, 总词数=total_words)
        
        # 4. 保存到缓存
        self.save(hash_id, description, word_bank)
        
        return hash_id, word_bank, False
    
    def clear_cache(self, hash_id: str = None):
        """
        清理缓存
        
        Args:
            hash_id: 如指定则清理单个缓存，否则清理全部
        """
        if hash_id:
            cache_path = self.get_cache_path(hash_id)
            if cache_path.exists():
                cache_path.unlink()
                print(f"已删除缓存: {cache_path}")
        else:
            import shutil
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(exist_ok=True)
                print(f"已清空所有缓存: {self.cache_dir}")
    
    def list_cached(self) -> list:
        """列出所有已缓存的类别"""
        cached = []
        for sub_dir in self.cache_dir.iterdir():
            if sub_dir.is_dir():
                for cache_file in sub_dir.glob("*.json"):
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        cached.append({
                            "hash_id": data.get("hash_id", "")[:16] + "...",
                            "描述": data.get("类别描述", "")[:30] + "...",
                            "词数": data.get("统计信息", {}).get("总词数", 0),
                            "生成时间": data.get("生成时间", "")
                        })
                    except:
                        pass
        return cached
```

#### TextClassifier（文本分类器）
```python
# text_classifier.py
import re
from dataclasses import dataclass
from typing import List, Tuple, Dict

@dataclass
class MatchResult:
    """匹配结果"""
    hash_id: str
    category_desc: str
    similarity: float
    matched_words: List[Tuple[str, str]]  # (词, 位置)
    is_from_cache: bool
    
    def to_dict(self) -> dict:
        return {
            'hash_id': self.hash_id[:16] + "..." if len(self.hash_id) > 16 else self.hash_id,
            '类别描述': self.category_desc,
            '相似度': round(self.similarity, 4),
            '匹配词': [w for w, _ in self.matched_words],
            '匹配位置': dict(self.matched_words),
            '来自缓存': self.is_from_cache
        }


class TextClassifier:
    """
    轻量级关键词分类器
    
    特点：
    - 纯关键词匹配，无模型依赖
    - 支持hashtag加权
    - 匹配结果可解释
    """
    
    def __init__(self, hashtag_weight: float = 1.5, max_score: float = 1.0):
        self.hashtag_weight = hashtag_weight
        self.max_score = max_score
    
    def classify(
        self, 
        hash_id: str,
        word_bank: Dict[str, float], 
        text: str, 
        category_desc: str = "",
        is_from_cache: bool = False
    ) -> MatchResult:
        """
        对文本进行分类匹配
        
        Args:
            hash_id: 类别Hash ID
            word_bank: 词库 {词: 权重}
            text: 待分类文本
            category_desc: 类别描述（用于结果展示）
            is_from_cache: 是否来自缓存
        """
        # 提取hashtag
        hashtags = re.findall(r'#([^#\s]+)', text)
        normal_text = re.sub(r'#[^#\s]+', '', text)
        
        matched_words = []
        score = 0.0
        matched_word_set = set()  # 防止重复计分
        
        # 在hashtag中匹配（加权）
        for tag in hashtags:
            for word, weight in word_bank.items():
                if word in tag and word not in matched_word_set:
                    score += weight * self.hashtag_weight
                    matched_words.append((word, f"#{tag}"))
                    matched_word_set.add(word)
                    break
        
        # 在普通文本中匹配
        for word, weight in word_bank.items():
            if word in normal_text and word not in matched_word_set:
                score += weight
                matched_words.append((word, "正文"))
                matched_word_set.add(word)
        
        # 归一化
        final_score = min(score / self.max_score, 1.0) if self.max_score else min(score, 1.0)
        
        return MatchResult(
            hash_id=hash_id,
            category_desc=category_desc,
            similarity=final_score,
            matched_words=matched_words,
            is_from_cache=is_from_cache
        )
```

### 4.3 批量处理器
```python
# batch_processor.py
import json
from pathlib import Path
from typing import List, Dict

class BatchProcessor:
    """批量处理器"""
    
    def __init__(self, classifier: TextClassifier):
        self.classifier = classifier
    
    def process_directory(
        self, 
        hash_id: str,
        word_bank: Dict[str, float],
        category_desc: str,
        xmls_dir: str,
        threshold: float = 0.3,
        is_from_cache: bool = False
    ) -> List[Dict]:
        """
        批量处理目录下的所有JSON文件
        
        Args:
            hash_id: 类别Hash ID
            word_bank: 词库
            category_desc: 类别描述
            xmls_dir: JSON文件目录
            threshold: 相似度阈值，低于此值的会被过滤
            is_from_cache: 词库是否来自缓存
        """
        results = []
        xmls_path = Path(xmls_dir)
        
        for json_file in xmls_path.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # 分类
            match_result = self.classifier.classify(
                hash_id=hash_id,
                word_bank=word_bank,
                text=data.get('文案', ''),
                category_name=category_desc,
                is_from_cache=is_from_cache
            )
            
            if match_result.similarity >= threshold:
                results.append({
                    '文件名': json_file.name,
                    '作者': data.get('作者', ''),
                    '文案': data.get('文案', '')[:50] + '...',
                    '相似度': match_result.similarity,
                    '匹配词': match_result.matched_words,
                    'hash_id': hash_id[:8]
                })
        
        # 按相似度排序
        results.sort(key=lambda x: x['相似度'], reverse=True)
        return results
```

---

## 五、实施步骤

### 阶段一：基础框架搭建

#### 5.1.1 创建项目结构
```
dy_text_classifier/
├── __init__.py
├── category_cache_manager.py    # 词库缓存管理（含Hash计算）
├── text_classifier.py           # 分类器核心
├── batch_processor.py           # 批量处理
├── evaluator.py                 # 评估工具
└── cache/                       # 缓存目录（运行时生成）
    └── a3f7b2d8/                # Hash前8位作为子目录
        └── word_bank.json       # 词库缓存文件
```

#### 5.1.2 实现CategoryCacheManager
（代码见上文4.2节）

#### 5.1.3 实现TextClassifier
（代码见上文4.2节）

### 阶段二：集成与优化

#### 5.2.1 缓存策略优化
- **目录分层**：使用hash前8位创建子目录，避免单目录文件过多
- **缓存清理**：提供清理过期缓存的接口
- **缓存统计**：支持查看已缓存的类别列表

#### 5.2.2 错误处理
- **API失败降级**：如API调用失败，尝试使用相似描述的缓存
- **缓存损坏处理**：检测并自动重建损坏的缓存文件

---

## 六、使用示例

### 6.1 完整使用流程

```python
# main.py
from category_cache_manager import CategoryCacheManager
from text_classifier import TextClassifier
from batch_processor import BatchProcessor

# 1. 初始化缓存管理器
cache_manager = CategoryCacheManager(cache_dir="cache")

# 2. 定义类别描述
category_desc = "拼多多电商平台运营相关，包括店铺运营、发货、直通车、活动报名等"

# 3. 获取或创建词库（自动处理缓存）
hash_id, word_bank, is_from_cache = cache_manager.get_or_create(
    description=category_desc,
    total_words=20
)
print(f"Hash ID: {hash_id}")
print(f"来自缓存: {is_from_cache}")
print(f"词库大小: {len(word_bank)}")

# 4. 初始化分类器
classifier = TextClassifier(hashtag_weight=1.5)

# 5. 单条测试
test_text = "#老陶电商 #拼多多运营 能发货的抓紧时间去报名活动和打开直通车！"
result = classifier.classify(
    hash_id=hash_id,
    word_bank=word_bank,
    text=test_text,
    category_desc=category_desc,
    is_from_cache=is_from_cache
)
print(f"\n相似度: {result.similarity}")
print(f"匹配词: {result.matched_words}")

# 6. 批量处理（使用相同的hash_id和word_bank）
processor = BatchProcessor(classifier)
results = processor.process_directory(
    hash_id=hash_id,
    word_bank=word_bank,
    category_desc=category_desc,
    xmls_dir="/home/yka-003/workspace/caidao/ut/xmls",
    threshold=0.3,
    is_from_cache=is_from_cache
)

# 7. 查看Top5
print("\n最相似的5条文案:")
for i, item in enumerate(results[:5], 1):
    print(f"{i}. [{item['相似度']:.2f}] {item['文案']}")

# 8. 查看缓存统计
print(f"\n已缓存类别数: {len(cache_manager.list_cached())}")
```

### 6.2 缓存复用示例

```python
# 第二次使用相同类别描述，将直接命中缓存
cache_manager2 = CategoryCacheManager(cache_dir="cache")

# 相同的描述会计算出相同的hash
hash_id2, word_bank2, is_from_cache2 = cache_manager2.get_or_create(
    description="拼多多电商平台运营相关，包括店铺运营、发货、直通车、活动报名等",
    total_words=20
)

print(f"Hash ID相同: {hash_id == hash_id2}")  # True
print(f"来自缓存: {is_from_cache2}")  # True，无需API调用
```

### 6.3 管理缓存

```python
# 列出所有缓存
cached = cache_manager.list_cached()
for item in cached:
    print(f"{item['hash_id']}: {item['描述']} ({item['词数']}词)")

# 删除特定缓存
cache_manager.clear_cache(hash_id="a3f7b2d8e5c9f1a4...")

# 清空所有缓存
cache_manager.clear_cache()
```

---

## 七、性能指标

### 7.1 体积预算
| 组件 | 体积估算 |
|-----|---------|
| helper_synonym.py | ~15KB |
| category_cache_manager.py | ~8KB |
| text_classifier.py | ~5KB |
| 单个类别缓存（JSON） | ~2-5KB |
| jieba分词（可选） | ~10MB |
| **总计（不含jieba）** | **< 100KB** ✅ |
| **总计（含jieba）** | **~10MB** ✅ |

### 7.2 性能指标
| 场景 | 目标值 |
|-----|-------|
| 首次调用（含API） | < 5s |
| 缓存命中调用 | < 10ms |
| 单条分类 | < 5ms |
| 批量处理（100条） | < 1s |
| 内存占用 | < 50MB |

### 7.3 缓存命中率
| 场景 | 预期命中率 |
|-----|-----------|
| 相同类别重复处理 | > 95% |
| 相似描述不同批次 | 取决于描述相似度 |

---

## 八、测试方案

### 8.1 单元测试
```python
# tests/test_cache_manager.py
def test_hash_consistency():
    """测试Hash计算一致性"""
    desc = "拼多多电商运营"
    hash1 = CategoryCacheManager.compute_hash(desc)
    hash2 = CategoryCacheManager.compute_hash(desc)
    assert hash1 == hash2
    assert len(hash1) == 32

def test_cache_roundtrip():
    """测试缓存读写"""
    manager = CategoryCacheManager(cache_dir="test_cache")
    hash_id = "test_hash_123"
    word_bank = {"拼多多": 1.0, "PDD": 0.6}
    
    # 保存
    manager.save(hash_id, "测试描述", word_bank)
    
    # 读取
    loaded = manager.load(hash_id)
    assert loaded == word_bank
    
    # 清理
    manager.clear_cache(hash_id)

def test_cache_hit():
    """测试缓存命中"""
    manager = CategoryCacheManager(cache_dir="test_cache")
    desc = "测试类别描述"
    
    # 首次调用（模拟，不实际调用API）
    hash_id = CategoryCacheManager.compute_hash(desc)
    manager.save(hash_id, desc, {"测试": 1.0})
    
    # 检查存在性
    assert manager.exists(hash_id)
```

### 8.2 集成测试
1. **端到端测试**：描述 → Hash → 缓存 → 分类 → 结果
2. **缓存复用测试**：相同描述多次调用，验证只调用一次API
3. **批量测试**：处理100条文案，检查性能

---

## 九、部署清单

### 9.1 文件清单
```
dy_text_classifier/
├── __init__.py                   # 包初始化
├── category_cache_manager.py     # 词库缓存管理（核心）
├── text_classifier.py            # 分类器核心
├── batch_processor.py            # 批量处理
├── evaluator.py                  # 评估工具
└── cache/                        # 缓存目录（运行时生成）
```

### 9.2 依赖清单
```
# requirements.txt（最小依赖）
# 无外部依赖，仅Python标准库

# requirements-full.txt（完整功能）
jieba>=0.42.1  # 中文分词（可选）
```

### 9.3 缓存目录结构
```
cache/
├── a3f7b2d8/                     # Hash前8位
│   └── word_bank.json            # 词库文件
├── b5e9c1f2/
│   └── word_bank.json
└── ...
```

---

## 十、后续优化方向

### 10.1 短期优化
1. **缓存压缩**：对词库JSON进行压缩存储
2. **缓存过期**：支持设置缓存有效期
3. **相似描述匹配**：描述微调时复用已有缓存

### 10.2 长期优化
1. **多级缓存**：内存缓存 + 文件缓存
2. **增量更新**：支持手动添加/删除关键词
3. **多类别对比**：一次文本与多个类别的相似度对比

---

## 十一、风险评估

| 风险 | 概率 | 影响 | 应对措施 |
|-----|------|------|---------|
| LLM API调用失败 | 中 | 高 | 缓存机制确保已生成的不受影响 |
| 缓存文件损坏 | 低 | 中 | 读取时校验，损坏则重建 |
| 关键词覆盖不足 | 中 | 中 | 调整total_words参数，或人工补充 |
| Hash冲突 | 极低 | 高 | MD5冲突概率可忽略，可加盐值 |

---

## 十二、总结

本方案采用**纯关键词匹配 + LLM自动生成同义词 + Hash缓存**的技术路线：

### 核心机制
```
类别描述 → MD5 Hash → 检查缓存
                              ↓
    缓存命中 ←──────────── 缓存未命中
        ↓                       ↓
    直接读取              调用API生成
        ↓                       ↓
        └─────────┬─────────────┘
                  ↓
            关键词匹配引擎
                  ↓
            相似度结果
```

### 关键特性
1. **智能缓存**：相同描述自动复用，避免重复API调用
2. **极速响应**：缓存命中时<10ms，批量处理友好
3. **极简体积**：核心代码<100KB，满足<10MB严格要求
4. **零训练成本**：无需准备训练数据，即开即用
5. **可解释性强**：匹配结果清晰展示命中了哪些关键词

**下一步行动**：确认方案后，开始实施工作。

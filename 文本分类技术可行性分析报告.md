# 抖音视频文案分类技术可行性分析报告

## 一、需求概述

基于 `ut/xmls/*.json` 文件中的视频文案数据，实现一个**可变类别文本相似度判断系统**。

### 核心需求
| 约束项 | 要求 |
|-------|------|
| 语言 | Python |
| 输入 | 类别名称（如"拼多多"）+ 文案字典 |
| 输出 | 相似度阈值（0~1之间的浮点数）|
| 类别特性 | 动态可变（3D建模、麻将、电商等）|
| 训练要求 | 最好无需训练 |
| 精度要求 | 不需要太精确 |
| **体积约束** | **⚠️ 关键约束：总体积 < 10MB** |

### 数据样例
```json
{
  "作者": "@老陶电商",
  "文案": "#老陶电商 #拼多多运营 能发货的抓紧时间去报名活动和打开直通车！",
  "音乐": "@老陶电商创作的原声",
  "点赞": "344",
  "评论": "20",
  "收藏": "142",
  "分享": "124",
  "类型": "视频"
}
```

---

## 二、技术方案对比分析

### ⚠️ 体积约束排除方案

以下方案因体积超标被排除：

| 方案 | 模型体积 | 状态 |
|-----|---------|------|
| 腾讯AI Lab词向量 | ~1-2GB | ❌ 超出10MB限制 |
| Sentence-BERT | ~100MB+ | ❌ 超出10MB限制 |
| 哈工大词向量 | ~500MB+ | ❌ 超出10MB限制 |
| 中文RoBERTa | ~400MB | ❌ 超出10MB限制 |

---

### 方案1：关键词精确匹配（推荐指数：⭐⭐⭐⭐⭐）

**原理**：直接检查文案中是否包含类别关键词

**体积**：~50KB（纯代码，无模型文件）✅

```python
def 相似度计算(类别名称: str, 文案: str) -> float:
    关键词列表 = 获取类别关键词(类别名称)  # 如"拼多多": ["拼多多", "pdd", "拼夕夕"]
    匹配数 = sum(1 for 词 in 关键词列表 if 词 in 文案)
    return min(匹配数 / len(关键词列表), 1.0)
```

**优点**：
- ✅ 体积极小（<100KB）
- ✅ 无需训练，即开即用
- ✅ 类别完全可变，只需配置关键词
- ✅ 速度快，O(n)复杂度
- ✅ 可解释性强
- ✅ 零依赖（无需PyTorch/TensorFlow）

**缺点**：
- ❌ 无法处理语义相似但字面不同的情况（如"PDD"和"拼多多"）
- ❌ 需要人工维护关键词库

**适用场景**：体积敏感、关键词明确的场景

---

### 方案2：轻量级自训练词向量（推荐指数：⭐⭐⭐⭐）

**原理**：基于现有JSON数据自训练小型词向量模型

**体积**：~1-5MB（可配置词表大小）✅

```python
from gensim.models import Word2Vec

# 使用现有文案数据自训练（几十条即可）
文案列表 = [json.load(open(f))['文案'] for f in xml_files]
分词结果 = [jieba.lcut(文案) for 文案 in 文案列表]

# 训练超小模型（词表限制1000词，向量维度50）
model = Word2Vec(
    sentences=分词结果,
    vector_size=50,      # 低维度
    window=5,
    min_count=1,
    max_vocab_size=1000, # 限制词表大小
    sg=0
)
# 模型体积约 1000词 × 50维 × 4字节 ≈ 200KB
```

**优点**：
- ✅ 体积极小（<5MB）
- ✅ 针对特定领域数据训练，效果可能更好
- ✅ 能理解训练数据中的语义关系
- ✅ 无需外部预训练模型

**缺点**：
- ❌ 需要一定量的训练数据（至少50+条文案）
- ❌ 泛化能力有限（对未登录词效果差）
- ❌ 需要训练时间（几分钟到十几分钟）

**适用场景**：有历史数据积累、追求语义理解但资源受限

---

### 方案3：基于哈希的文本相似度（MinHash/LSH）（推荐指数：⭐⭐⭐⭐）

**原理**：使用局部敏感哈希（LSH）快速估算文本相似度

**体积**：~100KB（纯算法实现）✅

```python
from datasketch import MinHash

def 计算相似度(文本1: str, 文本2: str) -> float:
    m1, m2 = MinHash(), MinHash()
    
    # 分词后使用3-gram
    for 词 in jieba.lcut(文本1):
        m1.update(词.encode('utf-8'))
    
    for 词 in jieba.lcut(文本2):
        m2.update(词.encode('utf-8'))
    
    return m1.jaccard(m2)
```

**优点**：
- ✅ 体积极小
- ✅ 无需预训练模型
- ✅ 可处理近似的文本相似
- ✅ 适合大规模文本匹配

**缺点**：
- ❌ 效果不如词向量精准
- ❌ 需要调节参数（n-gram大小、hash数量）
- ❌ 依赖 datasketch 库

---

### 方案4：TF-IDF + 余弦相似度（推荐指数：⭐⭐⭐）

**原理**：基于统计的词频-逆文档频率，计算文本相似度

**体积**：~1MB（稀疏矩阵存储）✅

**缺点**：
- ❌ 需要一定量的文档集合来计算IDF
- ❌ 冷启动问题（初始数据不足时效果差）

---

### 方案5：基于 LLM API（推荐指数：⭐⭐）

**原理**：调用 GPT/Claude/文心一言等API进行零样本分类

**体积**：~50KB（仅HTTP调用代码）✅

**缺点**：
- ❌ 需要API Key和网络
- ❌ 有调用成本
- ❌ 延迟较高（API往返）
- ❌ 不适合批量处理

---

## 三、推荐方案：分层级混合策略

### 3.0 核心设计原则

```
┌─────────────────────────────────────────┐
│         体积约束下的设计原则             │
├─────────────────────────────────────────┤
│ 1. 优先使用纯算法，避免加载大型模型       │
│ 2. 如需模型，自训练小模型（<5MB）        │
│ 3. 关键词库是体积最优选择                │
│ 4. 依赖库要轻量，避免PyTorch/TensorFlow  │
└─────────────────────────────────────────┘
```

### 3.1 推荐架构（总体积 < 5MB）

```
┌─────────────────────────────────────────────────────────┐
│                    输入层                                │
│  类别名称: "拼多多"   +   文案: "#拼多多运营..."         │
└─────────────────────────────────────────────────────────┘
                           │
              ┌────────────┴────────────┐
              ▼                          ▼
    ┌──────────────────┐      ┌──────────────────┐
    │   关键词匹配      │      │  轻量语义匹配     │
    │   (核心模块)      │      │  (可选增强)       │
    ├──────────────────┤      ├──────────────────┤
    │ • 精确匹配 +0.5  │      │ • 小型同义词库    │
    │ • 部分匹配 +0.2  │      │ • 简化的词向量    │
    │ • 同义词扩展     │      │   (自训练<3MB)   │
    └────────┬─────────┘      └────────┬─────────┘
             │                          │
             └───────────┬──────────────┘
                         ▼
              ┌──────────────────┐
              │   加权融合        │
              │ 关键词70%+语义30% │
              └────────┬─────────┘
                       ▼
              ┌──────────────────┐
              │   相似度分数      │
              │   (0.0 ~ 1.0)    │
              └──────────────────┘

总体积估算：
- 代码：~100KB
- 关键词库：~200KB
- 小型词向量（可选）：~3MB
- 总计：< 3.5MB ✅
```

### 3.2 核心代码实现（<10MB完整方案）

```python
# 文件: dy_text_classifier.py
# 体积: <500行代码，零重型依赖

import json
import re
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass

# ============ 核心配置 ============

@dataclass
class 分类器配置:
    """
    分类器配置类 - 完全可定制
    
    示例：
        拼多多配置 = 分类器配置(
            类别名称="拼多多",
            核心关键词=["拼多多", "PDD", "pdd", "拼夕夕"],
            扩展关键词=["多多", "拼多", "pdd运营"],
            同义词映射={
                "拼多多": ["pdd", "PDD", "拼夕夕"],
                "店铺": ["店", "商铺", "网店"]
            },
            hashtag权重=1.5,  # #话题 内的关键词权重更高
            语义相似度启用=False  # 体积敏感时关闭
        )
    """
    类别名称: str
    核心关键词: List[str]
    扩展关键词: List[str] = None
    同义词映射: Dict[str, List[str]] = None
    hashtag权重: float = 1.5
    语义相似度启用: bool = False
    核心词权重: float = 0.6
    扩展词权重: float = 0.3


class 轻量级文本分类器:
    """
    轻量级文本分类器 - 总体积 < 5MB
    
    特点：
    - 纯Python实现，零重型依赖
    - 核心功能仅依赖jieba分词（~10MB，可选）
    - 关键词模式：<100KB
    - 增强模式（含小模型）：<5MB
    """
    
    def __init__(self, 启用语义模块: bool = False, 模型路径: str = None):
        """
        初始化分类器
        
        Args:
            启用语义模块: 是否启用轻量级语义匹配（会增加~3MB体积）
            模型路径: 自训练的小型词向量模型路径（可选）
        """
        self.语义模块 = None
        self.同义词库 = self._加载默认同义词库()
        
        if 启用语义模块 and 模型路径:
            self._加载轻量级语义模块(模型路径)
    
    def _加载默认同义词库(self) -> Dict[str, List[str]]:
        """
        加载默认同义词库（体积小，内置）
        
        实际项目中可扩展为从文件加载
        """
        return {
            "拼多多": ["pdd", "PDD", "拼夕夕", "多多"],
            "淘宝": ["tb", "TB", "某宝"],
            "抖音": ["dy", "DY", "tiktok"],
            "电商": ["网店", "店铺", "卖家", "商家"],
            "直播": ["直播带货", "直播间", "主播"],
        }
    
    def _加载轻量级语义模块(self, 模型路径: str):
        """加载自训练的小型词向量模型"""
        try:
            from gensim.models import Word2Vec
            self.语义模块 = Word2Vec.load(模型路径)
        except Exception as e:
            print(f"语义模块加载失败: {e}，将仅使用关键词匹配")
    
    def 计算相似度(self, 配置: 分类器配置, 文案: str) -> Dict[str, float]:
        """
        计算文案与类别的相似度
        
        Returns:
            {
                '总分': float,           # 最终相似度 (0.0-1.0)
                '关键词分': float,       # 关键词匹配分数
                '语义分': float,         # 语义相似分数（如启用）
                '匹配详情': dict         # 详细匹配信息
            }
        """
        # 1. 关键词匹配（核心）
        关键词分, 匹配详情 = self._关键词匹配(配置, 文案)
        
        # 2. 语义相似度（可选）
        语义分 = 0.0
        if 配置.语义相似度启用 and self.语义模块:
            语义分 = self._轻量级语义匹配(配置.类别名称, 文案)
        
        # 3. 加权融合
        if 配置.语义相似度启用 and self.语义模块:
            总分 = 关键词分 * 0.7 + 语义分 * 0.3
        else:
            总分 = 关键词分
        
        return {
            '总分': round(min(总分, 1.0), 4),
            '关键词分': round(关键词分, 4),
            '语义分': round(语义分, 4) if 配置.语义相似度启用 else None,
            '匹配详情': 匹配详情
        }
    
    def _关键词匹配(self, 配置: 分类器配置, 文案: str) -> tuple:
        """
        关键词匹配算法
        
        策略：
        - 核心关键词匹配：+0.4/词
        - 扩展关键词匹配：+0.2/词
        - #hashtag 内的关键词：权重 × 1.5
        - 同义词也算匹配
        """
        # 构建完整关键词库
        核心词库 = set(配置.核心关键词)
        扩展词库 = set(配置.扩展关键词 or [])
        
        # 展开同义词
        if 配置.同义词映射:
            for 词, 同义词列表 in 配置.同义词映射.items():
                if 词 in 核心词库:
                    核心词库.update(同义词列表)
                if 词 in 扩展词库:
                    扩展词库.update(同义词列表)
        
        # 提取hashtag内容（权重更高）
        hashtag内容 = re.findall(r'#([^#\s]+)', 文案)
        普通文案 = re.sub(r'#[^#\s]+', '', 文案)
        
        匹配详情 = {
            '核心匹配': [],
            '扩展匹配': [],
            'hashtag匹配': [],
            '同义词匹配': []
        }
        
        分数 = 0.0
        
        # 检查hashtag（权重×1.5）
        for 标签 in hashtag内容:
            for 词 in 核心词库:
                if 词 in 标签:
                    分数 += 配置.核心词权重 * 配置.hashtag权重
                    匹配详情['hashtag匹配'].append(f"#{标签}(含{词})")
                    break
            for 词 in 扩展词库:
                if 词 in 标签:
                    分数 += 配置.扩展词权重 * 配置.hashtag权重
                    匹配详情['hashtag匹配'].append(f"#{标签}(含{词})")
                    break
        
        # 检查普通文案
        for 词 in 核心词库:
            if 词 in 普通文案:
                分数 += 配置.核心词权重
                匹配详情['核心匹配'].append(词)
        
        for 词 in 扩展词库:
            if 词 in 普通文案:
                分数 += 配置.扩展词权重
                匹配详情['扩展匹配'].append(词)
        
        # 归一化（最多1.0）
        归一化分数 = min(分数, 1.0)
        
        return 归一化分数, 匹配详情
    
    def _轻量级语义匹配(self, 类别名称: str, 文案: str) -> float:
        """
        轻量级语义匹配（基于自训练小模型）
        
        注意：此功能需要加载Word2Vec模型（~3MB）
        """
        if not self.语义模块:
            return 0.0
        
        import numpy as np
        
        # 简单分词
        类别词 = [w for w in 类别名称 if w in self.语义模块.wv]
        文案词 = [w for w in 文案 if w in self.语义模块.wv]
        
        if not 类别词 or not 文案词:
            return 0.0
        
        # 计算平均向量
        类别向量 = np.mean([self.语义模块.wv[w] for w in 类别词], axis=0)
        文案向量 = np.mean([self.语义模块.wv[w] for w in 文案词], axis=0)
        
        # 余弦相似度
        norm1 = np.linalg.norm(类别向量)
        norm2 = np.linalg.norm(文案向量)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(np.dot(类别向量, 文案向量) / (norm1 * norm2))


# ============ 使用示例 ============

def 批量分类示例():
    """批量处理JSON文件的示例"""
    
    # 初始化分类器（关键词模式，<100KB）
    分类器 = 轻量级文本分类器(启用语义模块=False)
    
    # 配置类别
    电商配置 = 分类器配置(
        类别名称="拼多多电商",
        核心关键词=["拼多多", "pdd", "PDD", "拼夕夕"],
        扩展关键词=["运营", "店铺", "发货", "直通车", "活动"],
        hashtag权重=1.5
    )
    
    # 批量处理
    xmls_dir = Path("ut/xmls")
    结果列表 = []
    
    for json_file in xmls_dir.glob("*.json"):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        相似度结果 = 分类器.计算相似度(电商配置, data['文案'])
        
        结果列表.append({
            '文件': json_file.name,
            '文案': data['文案'][:50] + '...',
            '相似度': 相似度结果['总分'],
            '匹配': 相似度结果['匹配详情']
        })
    
    # 按相似度排序
    结果列表.sort(key=lambda x: x['相似度'], reverse=True)
    
    # 输出Top5
    print(f"与类别 '{电商配置.类别名称}' 最相似的5条文案：")
    for i, item in enumerate(结果列表[:5], 1):
        print(f"{i}. {item['文件']} - 相似度: {item['相似度']}")
        print(f"   文案: {item['文案']}")


if __name__ == "__main__":
    批量分类示例()
```

---

## 四、方案对比（含体积维度）

| 方案 | 体积 | 推荐指数 | 是否需要训练 | 效果 | 适用场景 |
|-----|------|---------|-------------|------|---------|
| **关键词匹配（推荐）** | **<100KB** | ⭐⭐⭐⭐⭐ | ❌ 无需 | 中等 | **体积极度敏感** |
| **自训练小模型** | **1-5MB** | ⭐⭐⭐⭐ | ✅ 需要 | 较好 | 有历史数据、追求效果 |
| **MinHash/LSH** | ~200KB | ⭐⭐⭐⭐ | ❌ 无需 | 中等 | 大规模去重场景 |
| TF-IDF | ~1MB | ⭐⭐⭐ | ✅ 需要 | 一般 | 不推荐 |
| 预训练大模型 | >100MB | ❌ | ❌ 无需 | 好 | **超出体积限制** |

---

## 五、实施建议

### 阶段一：最小可行产品（MVP）- 1天

**体积**：< 100KB
**方案**：纯关键词匹配

```python
# 核心代码（可独立运行，无任何依赖）
import json, re
from pathlib import Path

def 简单分类(类别: str, 关键词: list, 文案: str) -> float:
    return sum(0.3 for k in 关键词 if k in 文案)

# 使用
关键词 = ["拼多多", "pdd", "PDD", "拼夕夕", "多多"]
for f in Path("ut/xmls").glob("*.json"):
    data = json.load(f)
    score = 简单分类("拼多多", 关键词, data['文案'])
    if score > 0.3:
        print(f"{f.name}: {score}")
```

### 阶段二：增强版 - 2天

**体积**：< 3MB
**方案**：关键词 + 同义词扩展 + hashtag加权

```python
# 添加上述完整版代码
# 包含同义词库、hashtag识别、详细匹配日志
```

### 阶段三：智能版（可选）- 3天

**体积**：< 5MB
**方案**：关键词（70%）+ 自训练词向量（30%）

```python
# 使用历史数据训练小型Word2Vec
# 模型体积控制在3MB以内
```

---

## 六、体积控制清单

```
部署前体积检查：

□ 代码文件大小 < 50KB
□ 关键词库文件 < 200KB  
□ 词向量模型（如使用）< 5MB
□ 依赖库检查：
  □ jieba: ~10MB（可选，纯Python）
  □ gensim: ~20MB（仅在启用语义模块时）
  □ numpy: ~15MB（gensim依赖）
  
总计：
- 最小部署（纯关键词）: < 300KB ✅
- 标准部署（关键词+jieba）: ~10MB ✅
- 增强部署（+小型模型）: ~15MB ⚠️（需优化）

优化建议：
- 如体积敏感，可去掉jieba，使用简单字符串匹配
- 这样最小体积可控制在 < 500KB
```

---

## 七、最终推荐

### 主方案：纯关键词匹配 + 同义词扩展

**体积**：< 500KB（代码+数据）

**理由**：
1. ✅ 完全满足 < 10MB 体积约束
2. ✅ 无需任何训练
3. ✅ 类别完全动态可变
4. ✅ 实现简单，维护成本低
5. ✅ 速度快，毫秒级响应
6. ✅ 可解释性强，便于调试

### 备选方案：自训练微型词向量

**体积**：< 5MB

**适用**：有充足历史数据且需要语义理解能力时

---

## 八、快速启动

```bash
# 1. 创建目录
mkdir -p dy_classifier

# 2. 复制核心代码（上述代码保存为 classifier.py，约300行）

# 3. 运行（零依赖）
python classifier.py

# 总体积检查
du -sh dy_classifier/
# 预期输出: < 500KB
```

---

## 九、附录

### 依赖安装（可选增强）

```bash
# 最小部署（推荐）
# 无需任何安装，纯Python标准库

# 标准部署（添加中文分词）
pip install jieba  # ~10MB

# 增强部署（添加语义理解）
pip install gensim numpy  # ~35MB
```

### 参考资料

1. **体积优化指南**：https://docs.python.org/3/distutils/builtdist.html
2. **轻量级NLP工具**：https://github.com/fxsjy/jieba
3. **微型词向量训练**：https://radimrehurek.com/gensim/models/word2vec.html
4. **文本相似度算法**：https://en.wikipedia.org/wiki/String_metric
